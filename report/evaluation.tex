\section{Evaluation}
\label{sec:evaluation}
Due to the lack of real users presence, a comprehensive real life user feedback cannot be collected for evaluation. After completing the training, I've
presented an accuracy result, which is the ratio of the number of positive region data to unknown region data. Due to the faster elimination technique
described in section\ref{sec:technical}, my model quickly removed the unlabeled data (hence, label them with wither 1 or -1). The iterative learning process
is terminated based on the accuracy achieved by this process. More details and reasoning behind this approach can be found in\cite{huang2018optimization}.
%
\par
After completing the learning process, another evaluation accuracy result is printed, which is calculated as follows:
\[
    \alpha = \dfrac{\text{predicted labels equal to the user label}}{\text{total number of labeled data by user}}
\]
%
%
\begin{table}[h!]
    \begin{center}
        \caption{Evaluation Results}
        \label{tab:table1}
        \begin{tabular}{c|c|c|c|c|c} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
            \textbf{Epochs} & \textbf{Begin} & \textbf{end} & \textbf{bins} & \textbf{$\lambda$} & \textbf{$\alpha$}\\
            \hline
            44 & 1242233747 & 1664725001 & 1000 & 0.8 & 0.519\\
            62 & 416663000 & 1664725001 & 2000 & 0.6 & 0.713\\
            56 & 972219900 & 1664725001 & 2000 & 0.6 & 0.625\\
            40 & 416663000 & 972219900 & 2000 & 0.5 & 0.503\\
        \end{tabular}
    \end{center}
\end{table}
%
%
In the table~\ref{tab:table1}, corresponding evaluation accuracy is presented. Note that, in this case, the total number of CPU thread location was fixed at
10. Here, the accuracy drops if the $\lambda$ value reduced, which is evidential that, the algorithm was being stopped prematurely (with any required
training). Again, increasing the time range also helps to increase the accuracy. Real inspection on the time series visualization revealed that, this is because
of the higher number of negative label data which owes to the fixed bin size that eliminated some of the interesting data points.
\par
It is obvious that, measuring evaluation accuracy in this way is not representative of what actually being learned. Appropriate labels generated from an
exhaustive user case study would be helpful to actually measure the real performance of the recommender model.